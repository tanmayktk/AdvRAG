{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your doc\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    \"https://blog.langchain.dev/langchain-v0-1-0/\"\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantaniate embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantaniate LLM\n",
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "openai_llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1250,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False\n",
    ")\n",
    "#\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantaniate Vectorstore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma(embedding_function=embeddings,\n",
    "                     persist_directory=\"/content/drive/MyDrive/Vectorstore/chromadb\",\n",
    "                     collection_name=\"full_documents\")\n",
    "# Load and persist the split documents into the vectorstore\n",
    "vectorstore.add_documents(split_docs)\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Keyword / Sparse embeddings model\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "#\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Reranker — Cross Encoders\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Optional, Sequence\n",
    "from langchain.schema import Document\n",
    "from langchain.pydantic_v1 import Extra, root_validator\n",
    "\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "# from config import bge_reranker_large\n",
    "\n",
    "class BgeRerank(BaseDocumentCompressor):\n",
    "    model_name:str = 'BAAI/bge-reranker-large'\n",
    "    \"\"\"Model name to use for reranking.\"\"\"\n",
    "    top_n: int = 3\n",
    "    \"\"\"Number of documents to return.\"\"\"\n",
    "    model:CrossEncoder = CrossEncoder(model_name)\n",
    "    \"\"\"CrossEncoder instance to use for reranking.\"\"\"\n",
    "\n",
    "    def bge_rerank(self,query,docs):\n",
    "        model_inputs =  [[query, doc] for doc in docs]\n",
    "        scores = self.model.predict(model_inputs)\n",
    "        results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "        return results[:self.top_n]\n",
    "\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: Sequence[Document],\n",
    "        query: str,\n",
    "        callbacks: Optional[Callbacks] = None,\n",
    "    ) -> Sequence[Document]:\n",
    "        \"\"\"\n",
    "        Compress documents using BAAI/bge-reranker models.\n",
    "\n",
    "        Args:\n",
    "            documents: A sequence of documents to compress.\n",
    "            query: The query to use for compressing the documents.\n",
    "            callbacks: Callbacks to run during the compression process.\n",
    "\n",
    "        Returns:\n",
    "            A sequence of compressed documents.\n",
    "        \"\"\"\n",
    "        if len(documents) == 0:  # to avoid empty api call\n",
    "            return []\n",
    "        doc_list = list(documents)\n",
    "        _docs = [d.page_content for d in doc_list]\n",
    "        results = self.bge_rerank(query, _docs)\n",
    "        final_results = []\n",
    "        for r in results:\n",
    "            doc = doc_list[r[0]]\n",
    "            doc.metadata[\"relevance_score\"] = r[1]\n",
    "            final_results.append(doc)\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Contextual Compression Pipeline\n",
    "from langchain_community.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_transformers.long_context_reorder import LongContextReorder\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "#\n",
    "vs_retriever = vectorstore.as_retriever(search_kwargs={\"k\":10})\n",
    "#\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever,vs_retriever],\n",
    "                                       weight=[0.5,0.5])\n",
    "#\n",
    "\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "#\n",
    "reordering = LongContextReorder()\n",
    "#\n",
    "reranker = BgeRerank()\n",
    "#\n",
    "pipeline_compressor = DocumentCompressorPipeline(transformers=[redundant_filter,reordering,reranker])\n",
    "#\n",
    "compression_pipeline = ContextualCompressionRetriever(base_compressor=pipeline_compressor,\n",
    "                                                      base_retriever=ensemble_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display retrieved documents\n",
    "def pretty_print_docs(docs):\n",
    "  print(\n",
    "      f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n + {d.page_content}\" for i,d in enumerate(docs)])\n",
    "  )\n",
    "\n",
    "docs = compression_pipeline.get_relevant_documents(\"What are the major changes in v 0.1.0?\")\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an Advanced RAG\n",
    "from langchain.chains import RetrievalQA\n",
    "#\n",
    "qa_advanced = RetrievalQA.from_chain_type(llm=openai_llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=compression_pipeline,\n",
    "                                 return_source_documents=True)\n",
    "#\n",
    "qa_adv_response = qa_advanced(\"What are the major changes in v 0.1.0?\")  \n",
    "qa_adv_response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Test Set Generation\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "#\n",
    "#load documents again to avoid any kind of bias\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)\n",
    "len(documents)\n",
    "#\n",
    "#\n",
    "generator = TestsetGenerator.with_openai()\n",
    "#\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses using our Advanced RAG pipeline using the questions we’ve generated.\n",
    "adv_answers = []\n",
    "adv_contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = qa_advanced.invoke({\"query\" : question})\n",
    "  adv_answers.append(response[\"result\"])\n",
    "  adv_contexts.append([context.page_content for context in response['source_documents']])\n",
    "\n",
    "#wrap into huggingface dataset\n",
    "response_dataset_advanced_retrieval = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : adv_answers,\n",
    "    \"contexts\" : adv_contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})\n",
    "response_dataset_advanced_retrieval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_retrieval_results = evaluate(response_dataset_advanced_retrieval, metrics,raise_exceptions=False)\n",
    "advanced_retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
